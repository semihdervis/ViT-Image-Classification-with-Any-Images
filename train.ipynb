{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since betul2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\semih\\.cache\\huggingface\\datasets\\betul2\\default\\0.0.0\\c8b151b1a7bf91b4 (last modified on Thu Aug 22 16:40:50 2024).\n"
     ]
    }
   ],
   "source": [
    "# Ensure all necessary imports are present\n",
    "from transformers import ViTFeatureExtractor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset('betul2')\n",
    "ds = ds.rename_column('label', 'labels')\n",
    "\n",
    "# Split the dataset\n",
    "ds = ds['train'].train_test_split(test_size=0.1)\n",
    "train_val = ds['train'].train_test_split(test_size=0.1)\n",
    "ds['train'] = train_val['train']\n",
    "ds['validation'] = train_val['test']\n",
    "ds['test'] = ds['test']\n",
    "\n",
    "# Convert to DatasetDict\n",
    "dataset = DatasetDict(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature extractor initialization\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single example\n",
    "def process_example(example):\n",
    "    image = example['image']\n",
    "    # Convert the PIL image to RGB format\n",
    "    image = image.convert('RGB')\n",
    "    # Convert the PIL image to numpy array\n",
    "    image_array = np.array(image)\n",
    "    # Ensure the image is in (height, width, channels) format and rearrange to (channels, height, width)\n",
    "    if image_array.ndim == 3 and image_array.shape[-1] in [1, 3, 4]:  # last dimension is channels\n",
    "        image_array = np.moveaxis(image_array, -1, 0)\n",
    "    # Now the image should be in (channels, height, width) format\n",
    "    inputs = feature_extractor(image_array, return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[ 0.0039,  0.0039,  0.0353,  ...,  0.0039, -0.0431,  0.0353],\n",
      "          [ 0.0039,  0.0039,  0.0353,  ...,  0.0039, -0.0431,  0.0353],\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039, -0.0431,  0.0431],\n",
      "          ...,\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0275, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0196, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039]],\n",
      "\n",
      "         [[ 0.0039,  0.0039,  0.0353,  ...,  0.0039, -0.0118,  0.0353],\n",
      "          [ 0.0039,  0.0039,  0.0353,  ...,  0.0039, -0.0118,  0.0353],\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039, -0.0118,  0.0431],\n",
      "          ...,\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0275, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0196, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039]],\n",
      "\n",
      "         [[ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0118],\n",
      "          ...,\n",
      "          [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0275, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n",
      "          [-0.0196, -0.0275,  0.0039,  ...,  0.0039,  0.0039,  0.0039]]]]), 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test the process_example function\n",
    "processed_example = process_example(ds['train'][0])\n",
    "print(processed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform a batch of examples\n",
    "def transform(example_batch):\n",
    "    images = [x.convert('RGB') for x in example_batch['image']]\n",
    "    # Ensure each image is in the correct format\n",
    "    images = [np.moveaxis(np.array(img), -1, 0) if img.ndim == 3 and img.shape[-1] in [1, 3, 4] else np.array(img) for img in images]\n",
    "    inputs = feature_extractor(images, return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform a batch of examples\n",
    "def transform(example_batch):\n",
    "    images = [x.convert('RGB') for x in example_batch['image']]\n",
    "    # Ensure each image is in the correct format\n",
    "    images = [np.moveaxis(np.array(img), -1, 0) if np.array(img).ndim == 3 and np.array(img).shape[-1] in [1, 3, 4] else np.array(img) for img in images]\n",
    "    inputs = feature_extractor(images, return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transform function to the dataset\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\AppData\\Local\\Temp\\ipykernel_17884\\267560914.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metric\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['labels'].names\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base--v5\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\semih\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e14aa007d24c6a96c54185edd19c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\vit\\modeling_vit.py:252: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1088, 'grad_norm': 1.39084792137146, 'learning_rate': 0.00019731182795698925, 'epoch': 0.05}\n",
      "{'loss': 0.8508, 'grad_norm': 1.115374207496643, 'learning_rate': 0.0001946236559139785, 'epoch': 0.11}\n",
      "{'loss': 0.8677, 'grad_norm': 1.134900450706482, 'learning_rate': 0.00019193548387096775, 'epoch': 0.16}\n",
      "{'loss': 0.8366, 'grad_norm': 1.3971041440963745, 'learning_rate': 0.000189247311827957, 'epoch': 0.22}\n",
      "{'loss': 0.7954, 'grad_norm': 1.8755857944488525, 'learning_rate': 0.00018655913978494625, 'epoch': 0.27}\n",
      "{'loss': 0.7438, 'grad_norm': 2.1053378582000732, 'learning_rate': 0.00018387096774193548, 'epoch': 0.32}\n",
      "{'loss': 0.8199, 'grad_norm': 1.3887600898742676, 'learning_rate': 0.00018118279569892475, 'epoch': 0.38}\n",
      "{'loss': 0.8945, 'grad_norm': 5.668015956878662, 'learning_rate': 0.00017849462365591398, 'epoch': 0.43}\n",
      "{'loss': 0.673, 'grad_norm': 1.4448820352554321, 'learning_rate': 0.00017580645161290325, 'epoch': 0.48}\n",
      "{'loss': 0.7009, 'grad_norm': 2.706613063812256, 'learning_rate': 0.00017311827956989248, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae76666ff134523a5abb91d0c7d35e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7025018930435181, 'eval_accuracy': 0.7636363636363637, 'eval_runtime': 2.2207, 'eval_samples_per_second': 148.604, 'eval_steps_per_second': 18.913, 'epoch': 0.54}\n",
      "{'loss': 0.6874, 'grad_norm': 1.2416284084320068, 'learning_rate': 0.00017043010752688172, 'epoch': 0.59}\n",
      "{'loss': 0.7256, 'grad_norm': 1.7809841632843018, 'learning_rate': 0.00016774193548387098, 'epoch': 0.65}\n",
      "{'loss': 0.7037, 'grad_norm': 0.9938160181045532, 'learning_rate': 0.00016505376344086022, 'epoch': 0.7}\n",
      "{'loss': 0.6592, 'grad_norm': 0.8089711666107178, 'learning_rate': 0.00016236559139784946, 'epoch': 0.75}\n",
      "{'loss': 0.7134, 'grad_norm': 0.8656672835350037, 'learning_rate': 0.00015967741935483872, 'epoch': 0.81}\n",
      "{'loss': 0.7579, 'grad_norm': 0.8881009817123413, 'learning_rate': 0.00015698924731182796, 'epoch': 0.86}\n",
      "{'loss': 0.6383, 'grad_norm': 3.4207422733306885, 'learning_rate': 0.00015430107526881722, 'epoch': 0.91}\n",
      "{'loss': 0.7042, 'grad_norm': 1.1370435953140259, 'learning_rate': 0.00015161290322580646, 'epoch': 0.97}\n",
      "{'loss': 0.6456, 'grad_norm': 1.1546010971069336, 'learning_rate': 0.00014892473118279572, 'epoch': 1.02}\n",
      "{'loss': 0.8243, 'grad_norm': 3.4705705642700195, 'learning_rate': 0.00014623655913978496, 'epoch': 1.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4793789017d14899b8f48d677653d8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6782025694847107, 'eval_accuracy': 0.7727272727272727, 'eval_runtime': 2.17, 'eval_samples_per_second': 152.073, 'eval_steps_per_second': 19.355, 'epoch': 1.08}\n",
      "{'loss': 0.5988, 'grad_norm': 1.0203512907028198, 'learning_rate': 0.00014354838709677422, 'epoch': 1.13}\n",
      "{'loss': 0.7319, 'grad_norm': 1.37605881690979, 'learning_rate': 0.00014086021505376346, 'epoch': 1.18}\n",
      "{'loss': 0.6869, 'grad_norm': 1.5589251518249512, 'learning_rate': 0.0001381720430107527, 'epoch': 1.24}\n",
      "{'loss': 0.5267, 'grad_norm': 0.7937272787094116, 'learning_rate': 0.00013548387096774193, 'epoch': 1.29}\n",
      "{'loss': 0.5889, 'grad_norm': 1.6712664365768433, 'learning_rate': 0.0001327956989247312, 'epoch': 1.34}\n",
      "{'loss': 0.752, 'grad_norm': 1.6246939897537231, 'learning_rate': 0.00013010752688172043, 'epoch': 1.4}\n",
      "{'loss': 0.5647, 'grad_norm': 0.6406051516532898, 'learning_rate': 0.0001274193548387097, 'epoch': 1.45}\n",
      "{'loss': 0.5053, 'grad_norm': 1.1794164180755615, 'learning_rate': 0.00012473118279569893, 'epoch': 1.51}\n",
      "{'loss': 0.6265, 'grad_norm': 1.325498104095459, 'learning_rate': 0.00012204301075268818, 'epoch': 1.56}\n",
      "{'loss': 0.5583, 'grad_norm': 0.9706299901008606, 'learning_rate': 0.00011935483870967743, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bc8fe874a34dfabbf28c2bbf1e8037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6262461543083191, 'eval_accuracy': 0.7818181818181819, 'eval_runtime': 1.8996, 'eval_samples_per_second': 173.718, 'eval_steps_per_second': 22.11, 'epoch': 1.61}\n",
      "{'loss': 0.6414, 'grad_norm': 0.6208717823028564, 'learning_rate': 0.00011666666666666668, 'epoch': 1.67}\n",
      "{'loss': 0.5549, 'grad_norm': 1.3186957836151123, 'learning_rate': 0.00011397849462365593, 'epoch': 1.72}\n",
      "{'loss': 0.6742, 'grad_norm': 2.5451791286468506, 'learning_rate': 0.00011129032258064515, 'epoch': 1.77}\n",
      "{'loss': 0.6711, 'grad_norm': 1.1998966932296753, 'learning_rate': 0.0001086021505376344, 'epoch': 1.83}\n",
      "{'loss': 0.5431, 'grad_norm': 1.3649810552597046, 'learning_rate': 0.00010591397849462365, 'epoch': 1.88}\n",
      "{'loss': 0.4781, 'grad_norm': 1.3462424278259277, 'learning_rate': 0.0001032258064516129, 'epoch': 1.94}\n",
      "{'loss': 0.6391, 'grad_norm': 1.4028325080871582, 'learning_rate': 0.00010053763440860215, 'epoch': 1.99}\n",
      "{'loss': 0.4725, 'grad_norm': 1.4146496057510376, 'learning_rate': 9.78494623655914e-05, 'epoch': 2.04}\n",
      "{'loss': 0.6137, 'grad_norm': 1.0350979566574097, 'learning_rate': 9.516129032258065e-05, 'epoch': 2.1}\n",
      "{'loss': 0.5596, 'grad_norm': 1.447263240814209, 'learning_rate': 9.247311827956989e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7bc32c9f784e0fb4626784c1ea2d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5823186635971069, 'eval_accuracy': 0.7848484848484848, 'eval_runtime': 1.9127, 'eval_samples_per_second': 172.529, 'eval_steps_per_second': 21.958, 'epoch': 2.15}\n",
      "{'loss': 0.4014, 'grad_norm': 0.9386455416679382, 'learning_rate': 8.978494623655914e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4946, 'grad_norm': 0.6845691204071045, 'learning_rate': 8.709677419354839e-05, 'epoch': 2.26}\n",
      "{'loss': 0.5419, 'grad_norm': 1.3128761053085327, 'learning_rate': 8.440860215053764e-05, 'epoch': 2.31}\n",
      "{'loss': 0.5379, 'grad_norm': 1.4756325483322144, 'learning_rate': 8.172043010752689e-05, 'epoch': 2.37}\n",
      "{'loss': 0.4826, 'grad_norm': 5.35335636138916, 'learning_rate': 7.903225806451613e-05, 'epoch': 2.42}\n",
      "{'loss': 0.4639, 'grad_norm': 0.6271783709526062, 'learning_rate': 7.634408602150538e-05, 'epoch': 2.47}\n",
      "{'loss': 0.5326, 'grad_norm': 2.0034284591674805, 'learning_rate': 7.365591397849463e-05, 'epoch': 2.53}\n",
      "{'loss': 0.5112, 'grad_norm': 1.7605661153793335, 'learning_rate': 7.096774193548388e-05, 'epoch': 2.58}\n",
      "{'loss': 0.3687, 'grad_norm': 2.4579708576202393, 'learning_rate': 6.827956989247311e-05, 'epoch': 2.63}\n",
      "{'loss': 0.496, 'grad_norm': 1.9988293647766113, 'learning_rate': 6.559139784946236e-05, 'epoch': 2.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035a684c6b2c41068d601db61351c36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5916585922241211, 'eval_accuracy': 0.806060606060606, 'eval_runtime': 1.913, 'eval_samples_per_second': 172.506, 'eval_steps_per_second': 21.955, 'epoch': 2.69}\n",
      "{'loss': 0.5579, 'grad_norm': 2.109804153442383, 'learning_rate': 6.290322580645161e-05, 'epoch': 2.74}\n",
      "{'loss': 0.5259, 'grad_norm': 1.6437660455703735, 'learning_rate': 6.021505376344086e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3856, 'grad_norm': 1.876278281211853, 'learning_rate': 5.752688172043011e-05, 'epoch': 2.85}\n",
      "{'loss': 0.4483, 'grad_norm': 2.5249826908111572, 'learning_rate': 5.4838709677419355e-05, 'epoch': 2.9}\n",
      "{'loss': 0.3772, 'grad_norm': 1.4441992044448853, 'learning_rate': 5.2150537634408605e-05, 'epoch': 2.96}\n",
      "{'loss': 0.4079, 'grad_norm': 5.1416497230529785, 'learning_rate': 4.9462365591397855e-05, 'epoch': 3.01}\n",
      "{'loss': 0.2888, 'grad_norm': 1.4264943599700928, 'learning_rate': 4.67741935483871e-05, 'epoch': 3.06}\n",
      "{'loss': 0.381, 'grad_norm': 1.0494979619979858, 'learning_rate': 4.408602150537635e-05, 'epoch': 3.12}\n",
      "{'loss': 0.4191, 'grad_norm': 2.061617612838745, 'learning_rate': 4.13978494623656e-05, 'epoch': 3.17}\n",
      "{'loss': 0.2714, 'grad_norm': 1.4831156730651855, 'learning_rate': 3.870967741935484e-05, 'epoch': 3.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76231fbdc01043c78d92e2cbe240f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5405390858650208, 'eval_accuracy': 0.793939393939394, 'eval_runtime': 1.9088, 'eval_samples_per_second': 172.887, 'eval_steps_per_second': 22.004, 'epoch': 3.23}\n",
      "{'loss': 0.2442, 'grad_norm': 1.3763409852981567, 'learning_rate': 3.602150537634409e-05, 'epoch': 3.28}\n",
      "{'loss': 0.3434, 'grad_norm': 4.556057929992676, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}\n",
      "{'loss': 0.4097, 'grad_norm': 0.828928530216217, 'learning_rate': 3.0645161290322585e-05, 'epoch': 3.39}\n",
      "{'loss': 0.3498, 'grad_norm': 3.557579755783081, 'learning_rate': 2.7956989247311828e-05, 'epoch': 3.44}\n",
      "{'loss': 0.2932, 'grad_norm': 2.2648048400878906, 'learning_rate': 2.5268817204301075e-05, 'epoch': 3.49}\n",
      "{'loss': 0.2564, 'grad_norm': 2.0926642417907715, 'learning_rate': 2.258064516129032e-05, 'epoch': 3.55}\n",
      "{'loss': 0.2702, 'grad_norm': 3.5984246730804443, 'learning_rate': 1.989247311827957e-05, 'epoch': 3.6}\n",
      "{'loss': 0.2812, 'grad_norm': 0.8931816816329956, 'learning_rate': 1.7204301075268818e-05, 'epoch': 3.66}\n",
      "{'loss': 0.3108, 'grad_norm': 2.550780773162842, 'learning_rate': 1.4516129032258066e-05, 'epoch': 3.71}\n",
      "{'loss': 0.2495, 'grad_norm': 1.849076747894287, 'learning_rate': 1.1827956989247313e-05, 'epoch': 3.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99cc3a9df3d4d718cabb5a0adf1852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5512449145317078, 'eval_accuracy': 0.8, 'eval_runtime': 1.8804, 'eval_samples_per_second': 175.491, 'eval_steps_per_second': 22.335, 'epoch': 3.76}\n",
      "{'loss': 0.3536, 'grad_norm': 1.8082451820373535, 'learning_rate': 9.13978494623656e-06, 'epoch': 3.82}\n",
      "{'loss': 0.2789, 'grad_norm': 0.4126892685890198, 'learning_rate': 6.451612903225806e-06, 'epoch': 3.87}\n",
      "{'loss': 0.2241, 'grad_norm': 0.8909844756126404, 'learning_rate': 3.763440860215054e-06, 'epoch': 3.92}\n",
      "{'loss': 0.2632, 'grad_norm': 2.3688242435455322, 'learning_rate': 1.0752688172043011e-06, 'epoch': 3.98}\n",
      "{'train_runtime': 163.7833, 'train_samples_per_second': 72.413, 'train_steps_per_second': 4.543, 'train_loss': 0.5439459455590094, 'epoch': 4.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =         4.0\n",
      "  total_flos               = 855959680GF\n",
      "  train_loss               =      0.5439\n",
      "  train_runtime            =  0:02:43.78\n",
      "  train_samples_per_second =      72.413\n",
      "  train_steps_per_second   =       4.543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99435f2b13ea4383965c1368b5c30262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.7902\n",
      "  eval_loss               =     0.5956\n",
      "  eval_runtime            = 0:00:02.43\n",
      "  eval_samples_per_second =    150.539\n",
      "  eval_steps_per_second   =     18.869\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Train the model\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Push to hub or create model card\n",
    "kwargs = {\n",
    "    \"finetuned_from\": model.config._name_or_path,\n",
    "    \"tasks\": \"image-classification\",\n",
    "    \"dataset\": 'custom brats layers',\n",
    "    \"tags\": ['image-classification'],\n",
    "}\n",
    "\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub('ðŸ» cheers', **kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
