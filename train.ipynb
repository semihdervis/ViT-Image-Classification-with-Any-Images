{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "DATASET_PATH = \"mnist\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and preprocess dataset\n",
    "ds = load_dataset(DATASET_PATH).rename_column('label', 'labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing splits and handle accordingly\n",
    "if 'train' in ds.keys() and 'validation' in ds.keys() and 'test' in ds.keys():\n",
    "    # Dataset already has train, validation, and test splits\n",
    "    dataset = DatasetDict({\n",
    "        'train': ds['train'],\n",
    "        'validation': ds['validation'],\n",
    "        'test': ds['test']\n",
    "    })\n",
    "elif 'train' in ds.keys() and 'validation' in ds.keys():\n",
    "    # Dataset has only train and validation splits, so create a test split from validation\n",
    "    ds = ds['validation'].train_test_split(test_size=0.5)\n",
    "    dataset = DatasetDict({\n",
    "        'train': ds['train'],\n",
    "        'validation': ds['test'],\n",
    "        'test': ds['test']\n",
    "    })\n",
    "elif 'train' in ds.keys() and 'test' in ds.keys():\n",
    "    # Dataset has only train and test splits, so create a validation split from train\n",
    "    train_val = ds['train'].train_test_split(test_size=0.1)\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_val['train'],\n",
    "        'validation': train_val['test'],\n",
    "        'test': ds['test']\n",
    "    })\n",
    "elif 'train' in ds.keys():\n",
    "    # Dataset only has a train split, so create both validation and test splits\n",
    "    ds = ds['train'].train_test_split(test_size=0.2)\n",
    "    train_val = ds['train'].train_test_split(test_size=0.1)\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_val['train'],\n",
    "        'validation': train_val['test'],\n",
    "        'test': ds['test']\n",
    "    })\n",
    "else:\n",
    "    raise ValueError(\"The dataset does not have a 'train' split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 54000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize feature extractor and model\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "labels = ds['train'].features['labels'].names\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(device)\n",
    "\n",
    "# Transform function\n",
    "def transform(example_batch):\n",
    "    images = [np.moveaxis(np.array(x.convert('RGB')), -1, 0) for x in example_batch['image']]\n",
    "    inputs = feature_extractor(images, return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "# Prepare dataset\n",
    "prepared_ds = ds.with_transform(transform)\n",
    "\n",
    "# Data collator\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "# Metric and compute function\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base--v5\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Model card creation\n",
    "kwargs = {\n",
    "    \"finetuned_from\": model.config._name_or_path,\n",
    "    \"tasks\": \"image-classification\",\n",
    "    \"dataset\": 'custom brats layers',\n",
    "    \"tags\": ['image-classification'],\n",
    "}\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub('üçª cheers', **kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84e5e058794854938207dfc632c736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4978531301021576,\n",
       " 'eval_accuracy': 0.8256130790190735,\n",
       " 'eval_runtime': 2.3023,\n",
       " 'eval_samples_per_second': 159.403,\n",
       " 'eval_steps_per_second': 19.98,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(prepared_ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4978531301021576, 'eval_accuracy': 0.8256130790190735, 'eval_runtime': 2.2373, 'eval_samples_per_second': 164.039, 'eval_steps_per_second': 20.561, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
